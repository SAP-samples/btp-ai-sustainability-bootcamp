{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02fc96bb",
   "metadata": {},
   "source": [
    "# LGP sound classification in AI Core -  Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67deac92-2aca-42ae-97fe-0a60fcf05873",
   "metadata": {},
   "source": [
    "The figure below summarizes the different steps one needs to go through to train and deploy a ML model in AI Core.\n",
    "\n",
    "<img src=\"../../../../resources/AICoreMLOps.png\" width=\"900\">\n",
    "\n",
    "In the [previous notebook](sound-classification-part1.ipynb), we have already took care of Section 0 and 1: we have connected a GitHub repository and a Docker Registry to the AI Core instance and we have created a resource group dedicated to our sound classification task. Moreover an AWS S3 storage bucket with our input sound data has been connected to this resource group.  \n",
    "\n",
    "In this notebook we will see how to **train, deploy and use the model for inference in SAP AI Core**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc2df7-2625-4257-aac8-45dd7a3e42a0",
   "metadata": {},
   "source": [
    "### Before getting started: color conventions\n",
    "\n",
    "The comments within the notebook will guide you to the required steps. Pay attention to the color conventions:\n",
    "\n",
    "* <span style=\"color:magenta\"> **Magenta text**  </span> indicates that you have to open certain json files and modify them according to your own set up, for instance you can be asked to enter credentials for a certain system, change names for the variables etc.  \n",
    "* <span style=\"color:blue\"> **Blue text**  </span> indicates that you have to execute commands on a terminal. \n",
    "* <span style=\"color:green\"> **Green text** </span> indicates that you are asked to modify something in the following notebook cell. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a334c-a693-4424-a9f1-84bb65f61890",
   "metadata": {},
   "source": [
    "## Create an AI API client instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8b394-c8ea-43fe-92f4-ec00d8ffe40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import json\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "import yaml\n",
    "from IPython.display import clear_output\n",
    "from pprint import pprint\n",
    "\n",
    "from ai_api_client_sdk.ai_api_v2_client import AIAPIV2Client\n",
    "from ai_api_client_sdk.models.artifact import Artifact\n",
    "from ai_api_client_sdk.models.status import Status\n",
    "from ai_api_client_sdk.models.target_status import TargetStatus\n",
    "from ai_api_client_sdk.models.parameter_binding import ParameterBinding\n",
    "from ai_api_client_sdk.models.input_artifact_binding import InputArtifactBinding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b79eb-084c-4c72-ac2a-5314432d1cc8",
   "metadata": {},
   "source": [
    "\n",
    "First of all, we need to create an AI API client instance, which will allow us to interact with our SAP AI Core tenant. You might remember we have done the same at the beginning of the [previous notebook](sound-classification-part1.ipynb). <span style=\"color:magenta\">Before executing the code, double check </span> **[aic_service_key.json](./files/aic_service_key.json)** <span style=\"color:magenta\">contains the correct credentials</span>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf34989",
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_service_key = \"./files/aic_service_key.json\" \n",
    "with open(aic_service_key) as ask:\n",
    "    aic_s_k = json.load(ask)\n",
    "\n",
    "ai_api_client = AIAPIV2Client(\n",
    "    base_url=aic_s_k[\"serviceurls\"][\"AI_API_URL\"] + \"/v2\",\n",
    "    auth_url=aic_s_k[\"url\"] + \"/oauth/token\",\n",
    "    client_id=aic_s_k['clientid'],\n",
    "    client_secret=aic_s_k['clientsecret']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ccfd54-63c4-4d8e-a603-dd870d461303",
   "metadata": {},
   "source": [
    "## Train the sound classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df452f0-d3af-426d-99f4-26502c14297e",
   "metadata": {},
   "source": [
    "### Create the training docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27a639-a90c-46f7-b164-b58b15ddc584",
   "metadata": {},
   "source": [
    "In order to execute a training pipeline in AI Core, you need to dockerize your training code and push the docker image to your docker registry. You should have already linked your docker registry to SAP AI Core during the previous exercises. <span style=\"color:blue\"> You should be then ready to build and push your docker image to the docker registry. To do that, open a terminal in Jupyter (go to Files at the top-left of the Jupyter Notebook, then select New > Terminal) and type the following commands line-by-line. Note that after the first command, you will be asked for a password to log in to your Docker. Copy and paste your Docker access token (do not use your docker password). After you have copied and pasted you will see that the cursor has not moved or changed, even though the token will have been entered. Click enter.  You should see a response Login Succeeded.<span>\n",
    " \n",
    "```sh\n",
    "cd YOUR_PATH_TO/btp-ai-sustainability-bootcamp/src/ai-models/predictive-maintenance/code/train\n",
    "docker login docker.io -u YOUR_DOCKER_USERNAME\n",
    "docker buildx build -o type=docker --platform=linux/amd64 -t YOUR_DOCKER_USERNAME/sound-train:latest .\n",
    "docker push docker.io/YOUR_DOCKER_USERNAME/sound-train:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e4b831-56ee-4b84-a63b-8cf7e2217058",
   "metadata": {},
   "source": [
    "### Create a training workflow and register it as an application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3afd9b1-7c71-4578-8d1a-edcb3ff3999c",
   "metadata": {},
   "source": [
    "After having prepared the docker image, you need to create a training workflow in the GitHub repository associated to our AI Core instance. The yaml file needs to be uploaded in a dedicated folder of the GitHub repository. Then you can register your application as shown below. Before executing the code, you need to:\n",
    "\n",
    "* <span style=\"color:magenta\">Edit your training workflow yaml file </span> in [./files/training_workflow.yaml](./files/training_workflow.yaml). It should point to your own AI scenario metadata, your own docker registry etc., so make sure you adapt the following line:\n",
    "    - line 37 - <span style=\"color:magenta\">\"docker.io/YOUR_DOCKER_USERNAME/sound-train:latest\"</span> -  enter your Docker username\n",
    "\n",
    "\n",
    "\n",
    "* <span style=\"color:blue\">Create a dedicated folder in your GitHub repository and load there the training workflow yaml file. To do that, navigate to the terminal tab you have previously opened on Jupyter and type the following commands line-by-line.\n",
    "\n",
    "```sh\n",
    "cd PATH_TO_YOUR_GITHUB_REPO\n",
    "mkdir workflows_sound\n",
    "cp ../btp-ai-sustainability-bootcamp/src/ai-models/predictive-maintenance/exercises/files/training_workflow.yaml \\\n",
    "workflows_sound\n",
    "git pull\n",
    "git add workflows_sound\n",
    "git commit -m \"add a new workflow folder\"\n",
    "git push\n",
    "```\n",
    "\n",
    "\n",
    "* <span style=\"color:magenta\"> Open </span>[./files/git_setup.json](./files/git_setup.json) <span style=\"color:magenta\"> and adjust the app section to reflect your workflow folder name, app name, and GitHub repository URL. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8237b-9e60-4479-812d-229b47ec1205",
   "metadata": {},
   "source": [
    "#### Check out available repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8816762-5f0c-43e2-86de-4b4a64f307c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_api_client.rest_client.get(\n",
    "    path=\"/admin/repositories\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959aaf2-6df3-4c43-8865-53f8ce0e0734",
   "metadata": {},
   "source": [
    "#### Create a new application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e2fce-f72c-4926-8172-8f40dcd78940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads your git_setup.json\n",
    "with open('./files/git_setup.json') as gs:\n",
    "    setup_json = json.load(gs)\n",
    "    \n",
    "# Registers the directory as app\n",
    "app_json = setup_json[\"app\"]\n",
    "response = ai_api_client.rest_client.post(\n",
    "    path=\"/admin/applications\",\n",
    "    body={\n",
    "        \"applicationName\": app_json[\"applicationName\"],\n",
    "        \"repositoryUrl\": app_json[\"repositoryUrl\"],\n",
    "        \"revision\": app_json[\"revision\"],\n",
    "        \"path\": app_json[\"path\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab940a-668f-4c41-afd5-df427a1afac8",
   "metadata": {},
   "source": [
    "It is always a good practice to check the synchronization of the workflows. \n",
    "Please, keep in mind that the synchronization is triggered by any change to the yaml files pushed to GitHub and that AI Core checks every 3 minutes for new files or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a39b34-fa42-428f-83fd-e356834b9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./files/git_setup.json') as gs:\n",
    "    setup_json = json.load(gs)\n",
    "app_json = setup_json[\"app\"]\n",
    "app_name = app_json[\"applicationName\"]\n",
    "\n",
    "ai_api_client.rest_client.get(\n",
    "    path=f\"/admin/applications/{app_name}/status\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df441c9a-f4f8-44a9-bff4-e2e9defe57fc",
   "metadata": {},
   "source": [
    "### Choose a scenario and register the input dataset as an artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444d0e6-24bf-49a3-aec2-382e387cab1a",
   "metadata": {},
   "source": [
    "We now need to create a second API client linked to our resource group. <span style=\"color:green\"> Please, remind  that if your are working with the free tier AI Core you can only choose the default resource group.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9a6db-f9ee-4807-bc47-3fafde08f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group = 'default'\n",
    "\n",
    "aic_service_key = \"./files/aic_service_key.json\" # ENSURE YOU HAVE THE FILE PLACED CORRECTLY\n",
    "with open(aic_service_key) as ask:\n",
    "    aic_s_k = json.load(ask)\n",
    "\n",
    "ai_api_lm = AIAPIV2Client(\n",
    "    base_url=aic_s_k[\"serviceurls\"][\"AI_API_URL\"] + \"/v2/lm\",\n",
    "    auth_url=aic_s_k[\"url\"] + \"/oauth/token\",\n",
    "    client_id=aic_s_k['clientid'],\n",
    "    client_secret=aic_s_k['clientsecret'],\n",
    "    resource_group=resource_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b914a-4352-41b8-be96-e2b9accc2da3",
   "metadata": {},
   "source": [
    "The workflow yaml file that we have uploaded on Github contains specifications for an AI scenario, which is created as soon as AI Core synchronizes with the Github repo. The scenario appears also in the AI Launchpad. \n",
    "The cell below registers our input data as an artifact under this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137e440-9e97-4559-82f5-7cbe42e187a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training_workflow.yaml\n",
    "training_workflow_file = './files/training_workflow.yaml'\n",
    "with open(training_workflow_file) as twf:\n",
    "    training_workflow = yaml.safe_load(twf)\n",
    "\n",
    "# Load scenario id from train_workflow.yaml\n",
    "scenario_id = training_workflow['metadata']['labels']['scenarios.ai.sap.com/id']\n",
    "#\n",
    "# Set the artifact configuration\n",
    "artifact = {\n",
    "        \"name\": \"sound-data\", # Modifiable name\n",
    "        \"kind\": Artifact.Kind.DATASET,\n",
    "    \n",
    "        \"url\": \"ai://sound-object-store/data\",  \n",
    "    \n",
    "        \"description\": \"Cutting machine sound clips for defect detection\",\n",
    "        \"scenario_id\": scenario_id\n",
    "    }\n",
    "# Store the artifact response to retrieve the id for the training configuration\n",
    "artifact_resp = ai_api_lm.artifact.create(**artifact)\n",
    "print(f\"Artifacts registered for {scenario_id} scenario!\")\n",
    "pprint(vars(artifact_resp))\n",
    "#\n",
    "# Checks if the message contains expected string\n",
    "assert artifact_resp.message == 'Artifact acknowledged'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848075f7",
   "metadata": {},
   "source": [
    "### Create a training configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19fd6d-9f74-4c15-81bf-872c47d97c98",
   "metadata": {},
   "source": [
    "Everything is now ready to create a training configuration. This will instruct AI Core about the scenario, the executable, and the input data we want to be used for the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1997de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training_workflow.yaml\n",
    "training_workflow_file =  \"./files/training_workflow.yaml\"\n",
    "with open(training_workflow_file) as twf:\n",
    "    training_workflow = yaml.safe_load(twf)\n",
    "    \n",
    "# Load scenario id from train_workflow.yaml\n",
    "scenario_id = training_workflow['metadata']['labels']['scenarios.ai.sap.com/id']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea70a91-3200-4b18-95e0-599a2b31e951",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Please wait ~ 5 minutes before you execute the following cell.**  </span> For the configuration to be created successfully, AI Core must have completed the synchronization with the Github repo where we have created the template. If Ai Core is not yet synced, you you will get an error. In that case, try again after a few minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_artifact_name = training_workflow['spec']['templates'][0]['inputs']['artifacts'][0]['name']\n",
    "executable_name = training_workflow['metadata']['name']\n",
    "\n",
    "artifact_binding = {\n",
    "    \"key\": input_artifact_name,\n",
    "    \"artifact_id\": vars(artifact_resp)['id']\n",
    "}\n",
    "\n",
    "train_configuration = {\n",
    "    \"name\": \"sound-training-configuration\",\n",
    "    \"scenario_id\": scenario_id,\n",
    "    \"executable_id\": executable_name,\n",
    "    \"parameter_bindings\": [],\n",
    "    \"input_artifact_bindings\": [ InputArtifactBinding(**artifact_binding) ]\n",
    "}\n",
    "\n",
    "# store the configuration response to access the id to create an execution\n",
    "train_config_resp = ai_api_lm.configuration.create(**train_configuration)\n",
    "pprint(vars(train_config_resp))\n",
    "\n",
    "assert train_config_resp.message == 'Configuration created'\n",
    "\n",
    "print(\"Configuration created for running the training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78176fd",
   "metadata": {},
   "source": [
    "### Create a training execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc5306-6a5f-4357-b65c-dc5733563765",
   "metadata": {},
   "source": [
    "Let's use the execution API to launch the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ef445",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_resp = ai_api_lm.execution.create(train_config_resp.id)\n",
    "pprint(vars(execution_resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa158f",
   "metadata": {},
   "source": [
    "#### Observe the training status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ddd7a-ba92-4153-8472-87d96da57f8b",
   "metadata": {},
   "source": [
    "We can also use the **execution.get** API to monitor the status of the training. This operation will take several minutes. Notice that the execution produces an output artifact, the trained model, which gets its own id, name and url. This artifact will be used as input for the model deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35210f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "while status != Status.COMPLETED and status != Status.DEAD:\n",
    "    # Sleep for 5 secs to avoid overwhelming the API with requests\n",
    "    time.sleep(5)\n",
    "    # Clear outputs to reduce clutter\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    execution = ai_api_lm.execution.get(execution_resp.id)\n",
    "    status = execution.status\n",
    "    print('...... execution status ......', flush=True)\n",
    "    print(f\"Training status: {execution.status}\")\n",
    "    pprint(f\"Training status details: {execution.status_details}\")\n",
    "\n",
    "if execution.status == Status.COMPLETED:\n",
    "    print(f\"Training complete for execution [{execution_resp.id}]!\")\n",
    "    output_artifact = execution.output_artifacts[0]\n",
    "    training_output = {\n",
    "        \"id\": output_artifact.id,\n",
    "        \"name\": output_artifact.name,\n",
    "        \"url\": output_artifact.url\n",
    "    }\n",
    "    with open('training_output.json', 'w') as fp: #Save the reference to the model stored in S3\n",
    "        json.dump(training_output, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c962b",
   "metadata": {},
   "source": [
    "#### Metrics and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91988534-ab13-40c2-a0f6-b1e869886650",
   "metadata": {},
   "source": [
    "The metrics.query API allow us to inspect the training performance. \n",
    "In our training code, we have registered as metrics objects the loss function and the accuracy metric on the training and validation steps at each epoch of the training process. \n",
    "The loss and metric behavior as a function of the epoch are commonly used to check if the model training proceeded as expected. Let's plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59ba54",
   "metadata": {},
   "source": [
    "Let's have a look at the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f406a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_id = execution_resp.id\n",
    "metric_resp = ai_api_lm.metrics.query(execution_ids=exec_id)\n",
    "\n",
    "for m in metric_resp.resources:\n",
    "    for metric in m.metrics:\n",
    "        print(metric.name)\n",
    "        print(metric.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae341a23",
   "metadata": {},
   "source": [
    "Let's calculate and plot the confusion matrix now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f250ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "for m in metric_resp.resources:\n",
    "    for custom_info in m.custom_info:\n",
    "        #print(custom_info.name)\n",
    "        #print(custom_info.value)\n",
    "        all_metrics.append(custom_info.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = ast.literal_eval(all_metrics[0])\n",
    "confusion_matrix = ast.literal_eval(all_metrics[1])\n",
    "print(confusion_matrix['classes'])\n",
    "print(confusion_matrix['cf_matrix'])\n",
    "\n",
    "classes = confusion_matrix['classes']\n",
    "cnf_matrix = confusion_matrix['cf_matrix']\n",
    "cnf_matrix = np.array(cnf_matrix)\n",
    "#print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ee21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = ast.literal_eval(all_metrics[0])\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,5))\n",
    "\n",
    "a = ast.literal_eval(training_metrics[0].get(\"loss\"))\n",
    "b = ast.literal_eval(training_metrics[1].get(\"val_loss\"))\n",
    "c = ast.literal_eval(training_metrics[2].get(\"accuracy\"))\n",
    "d = ast.literal_eval(training_metrics[3].get(\"val_accuracy\"))\n",
    "\n",
    "axs[0].plot(a)\n",
    "axs[0].plot(b)\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].legend(['Train', 'Validation'], prop={'size': 20})\n",
    "\n",
    "axs[1].plot(c)\n",
    "axs[1].plot(d)\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].legend(['Train', 'Validation'], prop={'size': 20})\n",
    "\n",
    "e=axs[0].set_xlabel('Epoch',fontsize=25)\n",
    "e=axs[0].set_ylabel('Loss',fontsize=25)\n",
    "e=axs[1].set_xlabel('Epoch',fontsize=25)\n",
    "e=axs[1].set_ylabel('Accuracy',fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f32609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,\n",
    "                      title='Confusion matrix with normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a8ae8",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee33a19-e474-45f0-8604-9fd46f561456",
   "metadata": {},
   "source": [
    "Now that the model is trained, le's see how the deployment works. The steps are similar to the ones we went through for the training phase: \n",
    "* create a docker image with the deployment code\n",
    "* add a serving workflow by adding a dedicated yaml file on the git repository\n",
    "* specify the scenario for our deployment\n",
    "* create a configuration and launch the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e80cf0-93c9-4b98-85fd-0f941f9597b2",
   "metadata": {},
   "source": [
    "### Create serving docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1819b-e556-4ed5-8d82-12daf05fe13b",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">In order to execute a deployment in AI Core, you need to dockerize your serving code and push the docker image to your docker registry. \n",
    "You can do so by executing the following commands:</span>\n",
    "\n",
    "```sh\n",
    "cd YOUR_PATH_TO/btp-ai-sustainability-bootcamp/src/ai-models/predictive-maintenance/code/infer\n",
    "docker login docker.io -u YOUR_DOCKER_USERNAME\n",
    "docker buildx build -o type=docker --platform=linux/amd64 -t YOUR_DOCKER_USERNAME/sound-serve:latest .\n",
    "docker push docker.io/YOUR_DOCKER_USERNAME/sound-serve:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc76b9-7d9a-471b-9169-492dfbd7a9cc",
   "metadata": {},
   "source": [
    "### Create serving workflow and register it in SAP AI Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6c811-4961-46c4-a47e-53447005a952",
   "metadata": {},
   "source": [
    "After having prepared the docker image, you need to create a serving workflow in the github repository associated to our AI Core instance. \n",
    "We can upload the yaml file needs in the same folder we have created for the training workflow. Before executing the code, you need to:\n",
    "\n",
    "* <span style=\"color:magenta\">Edit your serving workflow yaml file </span> in [./files/serving_workflow.yaml](./files/serving_workflow.yaml). \n",
    "It should point to your own AI scenario name, your own docker registry and your own docker image, so make sure you adapt the following line:\n",
    "    - line 36 - <span style=\"color:magenta\">\"docker.io/YOUR_DOCKER_USERNAME/sound-serve:latest\"</span> -  enter your Docker username\n",
    "\n",
    "\n",
    "\n",
    "* <span style=\"color:blue\">Copy the yaml file in your GitHub repository:</span> \n",
    "```sh\n",
    "cd PATH_TO_YOUR_GITHUB_REPO\n",
    "cp ../btp-ai-sustainability-bootcamp/src/ai-models/predictive-maintenance/exercises/files/serving_workflow.yaml \\\n",
    "workflows_sound\n",
    "git pull\n",
    "git add workflows_sound/serving_workflow.yaml \n",
    "git commit -m \"add a new serving template\"\n",
    "git push\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8296cd0-562c-4bf3-9b51-cd7040e55ef3",
   "metadata": {},
   "source": [
    "Let's check the synchronization of the new workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71142935-1b4b-4d57-a23b-730a52cf8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./files/git_setup.json') as gs:\n",
    "    setup_json = json.load(gs)\n",
    "app_json = setup_json[\"app\"]\n",
    "app_name = app_json[\"applicationName\"]\n",
    "\n",
    "ai_api_client.rest_client.get(\n",
    "    path=f\"/admin/applications/{app_name}/status\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8003db7-3bf7-4785-95c5-3b88ab92e249",
   "metadata": {},
   "source": [
    "Notice that in the serving yaml file we have specified the same AI scenario that we have created for the training template. As soon as the AI Core synchronizes with the Docker repo, a new serving executable will be then available under our sound classification scenario. This can also be double checked in AI Launchpad. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cbd412-c1ec-44f3-bbea-d6e041dd9929",
   "metadata": {},
   "source": [
    "### Create a serving configuration\n",
    "Everything is now ready to create a serving configuration. This will instruct AI Core about the scenario, the executable, and the input artifact (trained model) we want to be used for the deployment. \n",
    "\n",
    "<span style=\"color:red\"> **Please wait ~ 5 minutes before you execute the following cell.**  </span> For the configuration to be created successfully, AI Core must have completed the synchronization with the Github repo where we have created the template. If Ai Core is not yet synced, you you will get an error. In that case, try again after a few minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_workflow_file = \"./files/serving_workflow.yaml\"\n",
    "with open(serving_workflow_file) as swf:\n",
    "    serving_workflow = yaml.safe_load(swf)\n",
    "\n",
    "scenario_id = serving_workflow['metadata']['labels']['scenarios.ai.sap.com/id']\n",
    "input_artifact_name = serving_workflow['spec']['inputs']['artifacts'][0]['name']\n",
    "executable_name = serving_workflow['metadata']['name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ef27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_binding = {\n",
    "    \"key\": input_artifact_name,\n",
    "    \"artifact_id\": training_output[\"id\"]\n",
    "}\n",
    "\n",
    "serve_configuration = {\n",
    "    \"name\": \"sound-serving-configuration\",\n",
    "    \"scenario_id\": scenario_id,\n",
    "    \"executable_id\": executable_name,\n",
    "    \"parameter_bindings\": [],\n",
    "    \"input_artifact_bindings\": [ InputArtifactBinding(**artifact_binding) ]\n",
    "}\n",
    "\n",
    "serve_config_resp = ai_api_lm.configuration.create(**serve_configuration)\n",
    "\n",
    "assert serve_config_resp.message == 'Configuration created'\n",
    "\n",
    "pprint(vars(serve_config_resp))\n",
    "print(\"configuration for serving the model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ddbf78",
   "metadata": {},
   "source": [
    "We can now trigger the deployment and check its status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_resp = ai_api_lm.deployment.create(serve_config_resp.id)\n",
    "#pprint(vars(deployment_resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d478829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll deployment status\n",
    "status = None\n",
    "while status != Status.RUNNING and status != Status.DEAD:\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)\n",
    "    deployment = ai_api_lm.deployment.get(deployment_resp.id)\n",
    "    status = deployment.status\n",
    "    print('...... deployment status ......', flush=True)\n",
    "    print(deployment.status)\n",
    "    pprint(deployment.status_details)\n",
    "\n",
    "    if deployment.status == Status.RUNNING:\n",
    "        print(f\"Deployment with {deployment_resp.id} complete!\")\n",
    "\n",
    "# Allow some time for deployment URL to get ready\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad911f",
   "metadata": {},
   "source": [
    "## Using the deployed ML model\n",
    "\n",
    "The deployment creates an endpoint which we can submit new sounds to. \n",
    "The API will respond to each request with the result of the sound classification. \n",
    "Let's see how to use the model with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27959b",
   "metadata": {},
   "source": [
    "Let's define the local path to the sound dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "path_normal_sounds = glob.glob(\"../LGPsound/ok/*\")\n",
    "path_abnormal_sounds_1 = glob.glob(\"../LGPsound/anomaly1/*\")\n",
    "path_abnormal_sounds_2 = glob.glob(\"../LGPsound/anomaly2/*\")\n",
    "#print(path_normal_sounds)\n",
    "#print(path_abnormal_sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495fb8a4",
   "metadata": {},
   "source": [
    "In order to perform the inference step, let's transform one of the images into a binary string (this will constitute the body of the API call):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio \n",
    "Audio(path_abnormal_sounds_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "import base64\n",
    "import io\n",
    "from json import dumps\n",
    "\n",
    "ENCODING = 'utf-8'\n",
    "\n",
    "# first: reading the binary stuff\n",
    "# note the 'rb' flag\n",
    "# result: bytes\n",
    "with open(path_abnormal_sounds_1[0], 'rb') as open_file:\n",
    "    byte_content = open_file.read()\n",
    "\n",
    "# second: base64 encode read data\n",
    "# result: bytes (again)\n",
    "base64_bytes = b64encode(byte_content)\n",
    "\n",
    "# third: decode these bytes to text\n",
    "# result: string (in utf-8)\n",
    "base64_string = base64_bytes.decode(ENCODING)\n",
    "\n",
    "# optional: doing stuff with the data\n",
    "# result here: some dict\n",
    "raw_data = {\"sound\": base64_string}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e55651-d2b9-4561-93ff-8a8cea1741be",
   "metadata": {},
   "source": [
    "We can now post our request to the model endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34619fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{deployment.deployment_url}/v1/models/soundmodel:predict\"\n",
    "#endpoint = \"https://api.ai.prod.us-east-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d63266fa273b16e0/v1/models/soundmodel:predict\"\n",
    "\n",
    "print(endpoint)\n",
    "\n",
    "headers = {\"Authorization\": ai_api_lm.rest_client.get_token(),\n",
    "           'ai-resource-group': resource_group,\n",
    "           \"Content-Type\": \"application/json\"}\n",
    "response = requests.post(endpoint, headers=headers, json=raw_data)\n",
    "\n",
    "print('Inference result:', response.json())\n",
    "#pprint(vars(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26668a-8705-4706-a13d-c0a886084a4a",
   "metadata": {},
   "source": [
    "Once you have done playing with the API you can stop the deployment to save resources like so (this is really important when using the free tier AI Core because only one execution at a time is allowed): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f48aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_resp = ai_api_lm.deployment.modify(deployment_resp.id, target_status=TargetStatus.STOPPED)\n",
    "\n",
    "status = None\n",
    "while status != Status.STOPPED:\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)\n",
    "    deployment = ai_api_lm.deployment.get(deployment_resp.id)\n",
    "    status = deployment.status\n",
    "    print('...... killing deployment ......', flush=True)\n",
    "    print(f\"Deployment status: {deployment.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7531360e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
